{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a48dbbf-3357-46a8-9b13-486165dab4a7",
   "metadata": {},
   "source": [
    "# Intergalactic Impressions: How to Use Neural Style to Create SpaceX Impressionism\n",
    "\n",
    "### A Project by Noel McCann\n",
    "\n",
    "<img src=\"images/impression_space.png\" style=\"width:550px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e76a996-c0bf-487c-a209-98e8414bb923",
   "metadata": {},
   "source": [
    "AI is enabling content creation like never before. From LLMs dedicated to writing novels to using algorithms to create Drake songs from scratch, artists and creators are entering a new world of opportunities. Using the artistic style of Paul Gauguin, we will use his art style to reimagine images of SpaceX's Dragon initiative via Neural Style Transfer, an algorithm created by [Gatys et al. (2015).](https://arxiv.org/abs/1508.06576)\n",
    "\n",
    "## This project will:\n",
    "- Implement the neural style transfer algorithm \n",
    "- Generate Gauguin-esque images via the algorithm \n",
    "- Define a style cost function & a content cost function\n",
    "- Enable others to experiment with this tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c63947c-61e2-4546-92ce-17482dff6497",
   "metadata": {},
   "source": [
    "## 1. Let's get started!\n",
    "First, we will run the following code cell to import the necessary packages and dependencies to perform a Neural Style Transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eacd9c-93ef-4211-8a66-20fec926601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pprint\n",
    "from public_tests import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01dbbaa-e285-46fe-901e-2e42ed6e4c25",
   "metadata": {},
   "source": [
    "\n",
    "## 2. What is Neural Style Transfer, anyway?\n",
    "\n",
    "Neural Style Transfer is a deep learning optimization technique that is gaining popularity amongst content creators. This transfer merges two images: a <strong>\"content\" image (C)</strong> and a <strong>\"style\" image (S)</strong>. Once merged, a <strong>\"generated\" image (G)</strong> is created, which combines the \"content\" of image C with the \"style\" of image S. \n",
    "\n",
    "One popular example is a Monet-style depiction of the Louvre. This combines a stock photo of the museum (content image) with the impressionist style of Claude Monet (style image):\n",
    "\n",
    "<img src=\"images/louvre_generated.png\" style=\"width:750px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaac50e1-4256-407a-8e75-da1db92378c5",
   "metadata": {},
   "source": [
    "## 3 - VGG-19\n",
    "\n",
    "Today, we will use the VGG network from the [original NST white paper](https://arxiv.org/abs/1508.06576) published by the Visual Geometry Group at the University of Oxford in 2014. We're using VGG-19, a 19-layer version of the VGG network that has previously been trained on the ImageNet database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37577923-ea6d-4012-880a-0a2d5fd5a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(272)\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "img_size = 400\n",
    "vgg = tf.keras.applications.VGG19(include_top=False,\n",
    "                                  input_shape=(img_size, img_size, 3),\n",
    "                                  weights='pretrained-model/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "\n",
    "vgg.trainable = False\n",
    "pp.pprint(vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712ac8dc-c3bc-4236-8d27-51f1f6954fa0",
   "metadata": {},
   "source": [
    "## 4 - Neural Style Transfer (NST)\n",
    "\n",
    "We will tackle the Neural Style Transfer (NST) algorithm in three steps:\n",
    "\n",
    "- First, we'll deal with the content cost function to understand the processing time of the content image\n",
    "- Second, we'll build the style cost function to understand the processing time of the style image\n",
    "- Finally, we'll put it all together to get our generated image\n",
    "\n",
    "<a name='4-1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19c0f1e-1811-4d6f-8d9d-44d3e5d9f2d1",
   "metadata": {},
   "source": [
    "#### Approach generated images like Goldilocks\n",
    "\n",
    "Ideally, the generated image should match the content of its source image. Doing this requires identifying the right layer. Here's a brief rundown of the layer types.\n",
    "\n",
    "* Shallow layers of a ConvNet tend to detect lower-level features such as edges and simple textures.\n",
    "* Deep layers tend to detect higher-level features such as more complex textures and object classes. \n",
    "\n",
    "#### Choosing a \"middle\" layer :\n",
    "The level of layers will impact what is ultimately generated. We don't want generated images that go too deep (further from desired style) nor too shallow (computationally expensive), so let's go right down the middle with our chosen image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e632bd1-1a35-402a-95ab-0446cee31f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = Image.open(\"images/6_spacex.jpg\")\n",
    "print(\"The content image shows a SpaceX Dragon spacecraft flying over Mars.\")\n",
    "content_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c65578c-32a7-40f6-b654-bc5eed9907ea",
   "metadata": {},
   "source": [
    "Looks adventrous, doesn't it? Let's see how intensive it'll be to generate an image based on the content image via \"content cost\". This process will use TensorFlow, specifically its [tf.reduce_sum](https://www.tensorflow.org/api_docs/python/tf/reduce_sum), [tf.square](https://www.tensorflow.org/api_docs/python/tf/square) and [tf.subtract](https://www.tensorflow.org/api_docs/python/tf/subtract) features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7847cfb5-cc5f-467a-bc2d-b43a053d3bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_content_cost(content_output, generated_output):\n",
    "    \"\"\"\n",
    "    Computes the content cost\n",
    "    \n",
    "    Arguments:\n",
    "    a_C -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image C \n",
    "    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image G\n",
    "    \n",
    "    Returns: \n",
    "    J_content -- scalar that is being computed\n",
    "    \"\"\"\n",
    "    a_C = content_output[-1]\n",
    "    a_G = generated_output[-1]\n",
    "    \n",
    "    # Retrieving dimensions from a_G (â‰ˆ1 line)\n",
    "    m, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
    "    \n",
    "    # Unrolling our images to make processing more efficient\n",
    "    a_C_unrolled = tf.transpose(tf.reshape(a_C, shape=[m, -1, n_C]))\n",
    "    a_G_unrolled = tf.transpose(tf.reshape(a_G, shape=[m, -1, n_C]))\n",
    "    \n",
    "    # Computing the cost\n",
    "    J_content =  (1 / (4 * n_H * n_W * n_C)) * tf.reduce_sum(tf.square(tf.subtract(a_C_unrolled, a_G_unrolled)))\n",
    "    \n",
    "    return J_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f97960-e259-4b08-892b-42a5583c96e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "a_C = tf.random.normal([1, 4, 4, 3], mean=1, stddev=4)\n",
    "a_G = tf.random.normal([1, 4, 4, 3], mean=1, stddev=4)\n",
    "J_content = compute_content_cost([a_C], [a_G])\n",
    "J_content_0 = compute_content_cost([a_C], [a_C])\n",
    "\n",
    "print(\"J_content = \" + str(J_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f9e75-720b-4b3f-a23e-ce971f4e199a",
   "metadata": {},
   "source": [
    "The content cost takes a hidden layer activation of the neural network, and measures how different the source and generated images will be. The cost is 7.057 when rounded. Not bad for a first image generator!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e80f9b-9e3d-41da-bec7-ac70e14e7858",
   "metadata": {},
   "source": [
    "### Adding some style\n",
    "\n",
    "For the running example, we will use the following style image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9562f1ec-6fcc-4e74-9d4d-1b93d62ed212",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "style_source = Image.open(\"images/gauguin.jpg\")\n",
    "style_source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab78f4f2-7b42-4613-9e9e-2133b456f0f7",
   "metadata": {},
   "source": [
    "Paul Gauguin was famous for his portraits and paintings of everyday life in the French Polynesia. His impressionist paints utilize an experimental use of color, perfect to add a bit of spice to the SpaceX pictures.\n",
    "\n",
    "To familiarize the NST with his style of art, let's first look into the Gram Matrix, which will share more info related to the active filters and textures used for the art style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a7d75d-2017-4c61-b50a-e954758de523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(A):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    A -- matrix of shape (n_C, n_H*n_W)\n",
    "    \n",
    "    Returns:\n",
    "    GA -- Gram matrix of A, of shape (n_C, n_C)\n",
    "    \"\"\"  \n",
    "    GA = tf.matmul(A, A, transpose_b=True)\n",
    "    \n",
    "    return GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75104e4a-45ad-46a5-8128-3993ed4a0f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "A = tf.random.normal([3, 2 * 1], mean=1, stddev=4)\n",
    "GA = gram_matrix(A)\n",
    "\n",
    "print(\"GA = \\n\" + str(GA))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49451a56-e485-4b21-9fd1-590415ebafd1",
   "metadata": {},
   "source": [
    "Now we know the level of textures of colors that will be used, let's minimize the distance between the Gram matrix of the style image and the Gram matrix of the generated image. This will ensure the highest possible quality for any pictures we recreate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f911be03-ef0b-4ce0-b60f-0ef5063455d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layer_style_cost(a_S, a_G):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    a_S -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image S \n",
    "    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image G\n",
    "    \n",
    "    Returns: \n",
    "    J_style_layer -- tensor representing a scalar value, style cost defined above by equation (2)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from a_G\n",
    "    m, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
    "    \n",
    "    # Reshape the images from (n_H * n_W, n_C) to have them of shape (n_C, n_H * n_W)\n",
    "    a_S = tf.transpose(tf.reshape(a_S, shape=[-1, n_C]))\n",
    "    a_G = tf.transpose(tf.reshape(a_G, shape=[-1, n_C]))\n",
    "\n",
    "    # Computing gram_matrices for both images S and G\n",
    "    GS = gram_matrix(a_S)\n",
    "    GG = gram_matrix(a_G)\n",
    "    \n",
    "    # Computing the loss\n",
    "    J_style_layer = (1 / (4 * n_C **2 * (n_H * n_W) **2)) * tf.reduce_sum(tf.square(tf.subtract(GS, GG)))\n",
    "\n",
    "    \n",
    "    return J_style_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d826c-0b46-4920-9bf8-287c65e18703",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "a_S = tf.random.normal([1, 4, 4, 3], mean=1, stddev=4)\n",
    "a_G = tf.random.normal([1, 4, 4, 3], mean=1, stddev=4)\n",
    "J_style_layer_GG = compute_layer_style_cost(a_G, a_G)\n",
    "J_style_layer_SG = compute_layer_style_cost(a_S, a_G)\n",
    "\n",
    "print(\"J_style_layer = \" + str(J_style_layer_SG))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b00d55-c4f4-4638-8837-d4fd0c0e37f7",
   "metadata": {},
   "source": [
    "## 5. Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ba7aa0-c101-4511-9518-e877d3531486",
   "metadata": {},
   "source": [
    "With one layer fully captured, let's \"merge\" style costs from other layers. We'll keep it simple by giving each layer equal weight, and the weights add up to 1.\n",
    "\n",
    "It's time to grab the layer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875200e5-a017-432c-9741-e0eacf9b9918",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4170a9b2-273f-4627-a6af-5dc73649801f",
   "metadata": {},
   "source": [
    "Let's take a peek into one of those layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76344d06-7654-43bd-b3f7-b6bafee0a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.get_layer('block5_conv4').output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f212a3-1a48-453b-9323-ef26cfcd3760",
   "metadata": {},
   "source": [
    "We'll now create the style layers, all adding up to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b32edb4-cca0-4296-a562-d7b7134f516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLE_LAYERS = [\n",
    "    ('block1_conv1', 0.2),\n",
    "    ('block2_conv1', 0.2),\n",
    "    ('block3_conv1', 0.2),\n",
    "    ('block4_conv1', 0.2),\n",
    "    ('block5_conv1', 0.2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098bd821-d412-4f58-bc07-c74143847440",
   "metadata": {},
   "source": [
    "Looks good, doesn't it? Before we get to the meaty stuff, we'll calculate the compute style cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9710c285-6b73-4a9a-ba2f-94b62b9c5bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_style_cost(style_image_output, generated_image_output, STYLE_LAYERS=STYLE_LAYERS):\n",
    "    \"\"\"\n",
    "    Computes the overall style cost from several chosen layers\n",
    "    \n",
    "    Arguments:\n",
    "    style_image_output -- our tensorflow model\n",
    "    generated_image_output --\n",
    "    STYLE_LAYERS -- A python list containing:\n",
    "                        - the names of the layers we would like to extract style from\n",
    "                        - a coefficient for each of them\n",
    "    \n",
    "    Returns: \n",
    "    J_style -- tensor representing a scalar value, style cost defined above by equation (2)\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize the overall style cost\n",
    "    J_style = 0\n",
    "\n",
    "    # Set a_S to be the hidden layer activation from the layer we have selected.\n",
    "    # The last element of the array contains the content layer image, which must not to be used.\n",
    "    a_S = style_image_output[:-1]\n",
    "\n",
    "    # Set a_G to be the output of the choosen hidden layers.\n",
    "    # The last element of the array contains the content layer image, which must not to be used.\n",
    "    a_G = generated_image_output[:-1]\n",
    "    for i, weight in zip(range(len(a_S)), STYLE_LAYERS):  \n",
    "        # Compute style_cost for the current layer\n",
    "        J_style_layer = compute_layer_style_cost(a_S[i], a_G[i])\n",
    "\n",
    "        # Add weight * J_style_layer of this layer to overall style cost\n",
    "        J_style += weight[1] * J_style_layer\n",
    "   \n",
    "    return J_style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af794401-d05f-4616-aafc-5900b71c25dc",
   "metadata": {},
   "source": [
    "The deeper layers capture higher-level concepts, and the features in the deeper layers are less localized in the image relative to each other. If someone wants to emulate the style source closely, they should try choosing smaller weights for deeper layers and larger weights for the first layers.\n",
    "\n",
    "With all the costs now calculated, let's bring it home and get the total cost function. The total cost includes both the content cost and the style cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd65bedb-5554-45cf-825f-3ac5fdc78029",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def total_cost(J_content, J_style, alpha = 10, beta = 40):\n",
    "    \"\"\"\n",
    "    Computes the total cost function\n",
    "    \n",
    "    Arguments:\n",
    "    J_content -- content cost coded above\n",
    "    J_style -- style cost coded above\n",
    "    alpha -- hyperparameter weighting the importance of the content cost\n",
    "    beta -- hyperparameter weighting the importance of the style cost\n",
    "    \n",
    "    Returns:\n",
    "    J -- total cost as defined by the formula above.\n",
    "    \"\"\"\n",
    "    J = alpha * J_content + beta * J_style\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd3b5f1-cc84-4e9c-b4ab-d40f1ac0bdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "J_content = 0.2    \n",
    "J_style = 0.8\n",
    "J = total_cost(J_content, J_style)\n",
    "\n",
    "np.random.seed(1)\n",
    "print(\"J = \" + str(total_cost(np.random.uniform(0, 1), np.random.uniform(0, 1))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717b5145-cf02-4169-97d8-dcc4a1b35b55",
   "metadata": {},
   "source": [
    "## 6. We have lift off!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28357e8b-4386-4a33-902e-2470f74e6094",
   "metadata": {},
   "source": [
    "With these prechecks out of the way, we should now be able to get our generated images off the ground (no pun intended). \n",
    "\n",
    "The tool below enables us to:\n",
    "\n",
    "1. Load a content image\n",
    "2. Load a style image\n",
    "3. Randomly initialize an image to be generated\n",
    "4. Load the VGG19 model\n",
    "5. Compute all attached costs\n",
    "6. Train and optimize all generated images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e50dfb2-3501-4689-8e93-f336916c1fd0",
   "metadata": {},
   "source": [
    "#### Step 1: Load your content image\n",
    "\n",
    "Let's load and resize the image we will use for our content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cffccc-786e-494e-b72a-fe5f7e548762",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = np.array(Image.open(\"images/6_spacex.jpg\").resize((img_size, img_size)))\n",
    "content_image = tf.constant(np.reshape(content_image, ((1,) + content_image.shape)))\n",
    "\n",
    "print(content_image.shape)\n",
    "imshow(content_image[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab11cff-3d2e-42ec-9ff7-9911e3cf968d",
   "metadata": {},
   "source": [
    "#### Step 2: Load your style image\n",
    "\n",
    "Now it's time to load Paul Gauguin as our style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6adbf68-0970-44cb-8491-ac612905b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image =  np.array(Image.open(\"images/gauguin.jpg\").resize((img_size, img_size)))\n",
    "style_image = tf.constant(np.reshape(style_image, ((1,) + style_image.shape)))\n",
    "\n",
    "print(style_image.shape)\n",
    "imshow(style_image[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33c069e-c698-4370-8210-c86bf0ff3d94",
   "metadata": {},
   "source": [
    "#### Step 3: Initialize the generated image\n",
    "\n",
    "This will help us match the content of the generated image more quickly and efficient with that of the content image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45bdb26-eb7a-403b-a393-b1fbe0f6b46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_image = tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32))\n",
    "noise = tf.random.uniform(tf.shape(generated_image), 0, 0.5)\n",
    "generated_image = tf.add(generated_image, noise)\n",
    "generated_image = tf.clip_by_value(generated_image, clip_value_min=0.0, clip_value_max=1.0)\n",
    "\n",
    "print(generated_image.shape)\n",
    "imshow(generated_image.numpy()[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eeac28-d6d7-4e45-bf7f-741a11771676",
   "metadata": {},
   "source": [
    "#### Step 4: Setup the VGG19 Model \n",
    "\n",
    "Remember VGG? It is now coming into play to ensure we can get our image in the style we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809570cc-2154-4a6c-8898-687a2f96e2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_outputs(vgg, layer_names):\n",
    "    \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n",
    "    outputs = [vgg.get_layer(layer[0]).output for layer in layer_names]\n",
    "\n",
    "    model = tf.keras.Model([vgg.input], outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839111ad-4001-419c-962f-4bb9eeccd190",
   "metadata": {},
   "source": [
    "We define which layer to use as our content layer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b139294-8897-4654-b791-46d52bafd0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layer = [('block5_conv4', 1)]\n",
    "\n",
    "vgg_model_outputs = get_layer_outputs(vgg, STYLE_LAYERS + content_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc5dbe8-a6af-4aa4-9d3a-876519f69161",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_target = vgg_model_outputs(content_image)  # Content\n",
    "style_targets = vgg_model_outputs(style_image)     # Style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb820f3d-7d74-4e88-aa73-12fbd7fbd37d",
   "metadata": {},
   "source": [
    "#### Step 5: Calculating the costs \n",
    "\n",
    "Now the model is complete, we should see how efficient or expensive it will be to generate the content we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087ee865-a40d-481c-ad8e-5535283a98e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing content image, with a_C set as the hidden layer.  \n",
    "preprocessed_content =  tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32))\n",
    "a_C = vgg_model_outputs(preprocessed_content)\n",
    "\n",
    "# Set a_G to be the hidden layer activation from the same layer.\n",
    "a_G = vgg_model_outputs(generated_image)\n",
    "\n",
    "# Content cost\n",
    "J_content = compute_content_cost(a_C, a_G)\n",
    "\n",
    "print(J_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18daf635-9d78-4815-b95b-8a80f0930109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign \"style\" image as input\n",
    "preprocessed_style =  tf.Variable(tf.image.convert_image_dtype(style_image, tf.float32))\n",
    "a_S = vgg_model_outputs(preprocessed_style)\n",
    "\n",
    "# Style cost\n",
    "J_style = compute_style_cost(a_S, a_G)\n",
    "print(J_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a0f07d-8ac9-40a8-9965-2c253b1c86c6",
   "metadata": {},
   "source": [
    "Final steps to ensure our generated images appear once run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3bb099-913d-45e0-a6b3-36226e3c025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_0_1(image):\n",
    "    \"\"\"\n",
    "    Truncate all the pixels in the tensor to be between 0 and 1\n",
    "    \n",
    "    Arguments:\n",
    "    image -- Tensor\n",
    "    J_style -- style cost coded above\n",
    "\n",
    "    Returns:\n",
    "    Tensor\n",
    "    \"\"\"\n",
    "    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    \"\"\"\n",
    "    Converts the given tensor into a PIL image\n",
    "    \n",
    "    Arguments:\n",
    "    tensor -- Tensor\n",
    "    \n",
    "    Returns:\n",
    "    Image: A PIL image\n",
    "    \"\"\"\n",
    "    tensor = tensor * 255\n",
    "    tensor = np.array(tensor, dtype=np.uint8)\n",
    "    if np.ndim(tensor) > 3:\n",
    "        assert tensor.shape[0] == 1\n",
    "        tensor = tensor[0]\n",
    "    return Image.fromarray(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ff3776-e2c9-487f-8443-398458c0a7e9",
   "metadata": {},
   "source": [
    "With all of that set, let's get the transfer learning ready. This is the final step, and it's where the bulk of the artificial intelligence work will kick in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933478b-6fb7-4945-b9fa-9a08e838143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.03)\n",
    "\n",
    "@tf.function()\n",
    "def train_step(generated_image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # We will use the precomputed encoded images a_S and a_C to compute a_G\n",
    "\n",
    "        a_G = vgg_model_outputs(generated_image)\n",
    "        \n",
    "        # Style cost\n",
    "        J_style = compute_style_cost(a_S, a_G)\n",
    "\n",
    "        # Content cost\n",
    "        J_content = compute_content_cost(a_C, a_G)\n",
    "        # Total cost\n",
    "        J = total_cost(J_content, J_style)  \n",
    "        \n",
    "        \n",
    "    grad = tape.gradient(J, generated_image)\n",
    "\n",
    "    optimizer.apply_gradients([(grad, generated_image)])\n",
    "    generated_image.assign(clip_0_1(generated_image))\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd320a-41b4-43c0-8769-e9564e39650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_image = tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32))\n",
    "\n",
    "J1 = train_step(generated_image)\n",
    "print(J1)\n",
    "\n",
    "J2 = train_step(generated_image)\n",
    "print(J2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0949ca1e-6ac1-4afa-9712-ec0bb49d4e16",
   "metadata": {},
   "source": [
    "#### Step 6: Training Day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65afce9-65be-403e-a5bf-71f76a073f66",
   "metadata": {},
   "source": [
    "The following cell will generate the desired images. This can take a while depending on the PC or Mac you use. I used a MacBook Pro 14-inch (2021) M1 Max 24-Core for the production of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2b8e69-f336-4e1a-8f16-7e94d909a4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2501\n",
    "for i in range(epochs):\n",
    "    train_step(generated_image)\n",
    "    if i % 250 == 0:\n",
    "        print(f\"Epoch {i} \")\n",
    "    if i % 250 == 0:\n",
    "        image = tensor_to_image(generated_image)\n",
    "        imshow(image)\n",
    "        image.save(f\"output/image_{i}.jpg\")\n",
    "        plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccb7e20-ff4d-4ea9-b73e-f2a4092f2e06",
   "metadata": {},
   "source": [
    "If it all goes to plan, you will have a generated piece of art! Run the following cell to see the end results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fb9f31-682d-45bb-bd21-9c227e74c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the content, style, and finished generated images in one row\n",
    "fig = plt.figure(figsize=(16, 4))\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "imshow(content_image[0])\n",
    "ax.title.set_text('Content image')\n",
    "ax = fig.add_subplot(1, 3, 2)\n",
    "imshow(style_image[0])\n",
    "ax.title.set_text('Style image')\n",
    "ax = fig.add_subplot(1, 3, 3)\n",
    "imshow(generated_image[0])\n",
    "ax.title.set_text('Generated image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5206815-d875-4ed2-aeaf-a18c2dfe1491",
   "metadata": {},
   "source": [
    "Want to give it a try? Download the file in this GitHub project and add your images to the Images folder. From there, make the following changes in steps 1 and 2:\n",
    "\n",
    "From: \n",
    "content_image = np.array(Image.open(\"images/6_spacex.jpg\").resize((img_size, img_size)))\n",
    "\n",
    "style_image =  np.array(Image.open(\"images/gauguin.jpg\").resize((img_size, img_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05010a84-bcb3-4444-b72c-003587eabb66",
   "metadata": {},
   "source": [
    "To:\n",
    "\n",
    "content_image = np.array(Image.open(\"images/(your content).jpg\").resize((img_size, img_size)))\n",
    "\n",
    "style_image =  np.array(Image.open(\"(your styl)e.jpg\").resize((img_size, img_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90018d5-24f4-4213-9dda-d66777523163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
